{"cells":[{"cell_type":"markdown","metadata":{"id":"sWX_eEsQDvGU"},"source":["## CNN implementation"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-05-01T20:34:52.913983Z","iopub.status.busy":"2023-05-01T20:34:52.913424Z","iopub.status.idle":"2023-05-01T20:34:52.940299Z","shell.execute_reply":"2023-05-01T20:34:52.939428Z","shell.execute_reply.started":"2023-05-01T20:34:52.913944Z"},"id":"MLThUTooASMr","outputId":"cf081925-1c41-4b90-f336-ef21492497ed","trusted":true},"outputs":[],"source":["import matplotlib.pyplot as plt\n","from PIL import Image\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"-5C-UJ9CPOEi"},"source":["## 1. Load Libraries"]},{"cell_type":"markdown","metadata":{"id":"ts2PphD5DvGd"},"source":["## 2. Load Dataset"]},{"cell_type":"markdown","metadata":{"id":"Q06Dmq6bn4SX"},"source":["## Extracting faces from train videos"]},{"cell_type":"markdown","metadata":{"id":"LOsuoq48DvGh"},"source":["## 3. Training Visualization "]},{"cell_type":"markdown","metadata":{"id":"8TviGPXlDvGj"},"source":["## 4. Load Weights"]},{"cell_type":"markdown","metadata":{"id":"PWpZJ0AgDvGk"},"source":["## 5. Data Exploration"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-cpbLVZvDvGl"},"outputs":[],"source":["#plt.bar(range(2), [len(train_files_orig),len(train_files_fake)])"]},{"cell_type":"markdown","metadata":{"id":"hqjicuj9DvGm"},"source":["## 6. Image Sample"]},{"cell_type":"markdown","metadata":{"id":"seWwuJqqDvGo"},"source":["## 7. Pre-process the Data"]},{"cell_type":"markdown","metadata":{"id":"h4yVJhClDvGo"},"source":["### Tensor Creation"]},{"cell_type":"markdown","metadata":{"id":"sDc9oNSDDvGq"},"source":["### Image Augmentation For The Fake Photos"]},{"cell_type":"markdown","metadata":{"id":"MDb435uS6yIg"},"source":["# Data generator"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-05-01T20:35:02.034183Z","iopub.status.busy":"2023-05-01T20:35:02.033204Z","iopub.status.idle":"2023-05-01T20:35:09.431353Z","shell.execute_reply":"2023-05-01T20:35:09.430262Z","shell.execute_reply.started":"2023-05-01T20:35:02.034128Z"},"id":"K_jxwdCP4NjY","outputId":"bd44bf4e-e0e4-46e6-d7d5-fbf04d18f845","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 1615 images belonging to 2 classes.\n","Found 409 images belonging to 2 classes.\n"]}],"source":["# create and configure augmented image generator\n","from keras.preprocessing.image import ImageDataGenerator\n","train_gen = ImageDataGenerator(\n","    rescale=1./255\n","    #width_shift_range=0.2,  # randomly shift images horizontally (10% of total width)\n","    #height_shift_range=0.2,  # randomly shift images vertically (10% of total height)\n","    #horizontal_flip=True,\n","    #vertical_flip = True,\n","    #shear_range=0.15,\n","    #zoom_range=0.2,\n",") # randomly flip images horizontally\n","train_generator = train_gen.flow_from_directory(\n","        # This is the target directory\n","        \"/kaggle/input/deep-fake-gp/train\",\n","        # All images will be resized to 150x150\n","        target_size=(224, 224),\n","        batch_size=32,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary',\n","        #color_mode=\"grayscale\"\n","        )\n","val_gen = ImageDataGenerator(rescale=1./255)\n","val_generator = val_gen.flow_from_directory(\n","        # This is the target directory\n","        \"/kaggle/input/deep-fake-gp/val\",\n","        # All images will be resized to 150x150\n","        target_size=(224, 224),\n","        batch_size=32,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary',\n","        #color_mode=\"grayscale\"\n","        )"]},{"cell_type":"markdown","metadata":{"id":"YecnnBsxDvGr"},"source":["## 8. Custom Architecture"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-05-01T20:35:16.253503Z","iopub.status.busy":"2023-05-01T20:35:16.252778Z","iopub.status.idle":"2023-05-01T20:35:48.827332Z","shell.execute_reply":"2023-05-01T20:35:48.826380Z","shell.execute_reply.started":"2023-05-01T20:35:16.253464Z"},"id":"ir1pYfvfDvGs","outputId":"e374caef-3ba2-4b6d-ec2a-d8ba40ba223c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (2.11.0)\n","Collecting protobuf<3.20,>=3.9.2\n","  Downloading protobuf-3.19.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.1.21)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from tensorflow) (23.0)\n","Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (4.4.0)\n","Requirement already satisfied: keras<2.12,>=2.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: tensorboard<2.12,>=2.11 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.11.2)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.51.1)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (0.29.0)\n","Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (15.0.6.1)\n","Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (3.8.0)\n","Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.21.6)\n","Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (2.2.0)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from tensorflow) (59.8.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.7/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.6.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.2.3)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (1.35.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.28.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.2.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (4.11.4)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.14)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2022.12.7)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.11.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n","Installing collected packages: protobuf\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 21.12.2 requires cupy-cuda115, which is not installed.\n","tfx-bsl 1.12.0 requires google-api-python-client<2,>=1.7.11, but you have google-api-python-client 2.83.0 which is incompatible.\n","tfx-bsl 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","tensorflow-transform 1.12.0 requires pyarrow<7,>=6, but you have pyarrow 5.0.0 which is incompatible.\n","onnx 1.13.1 requires protobuf<4,>=3.20.2, but you have protobuf 3.19.6 which is incompatible.\n","apache-beam 2.44.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.6 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed protobuf-3.19.6\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mCollecting keras-layer-normalization\n","  Downloading keras-layer-normalization-0.16.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-layer-normalization) (1.21.6)\n","Building wheels for collected packages: keras-layer-normalization\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.16.0-py3-none-any.whl size=4668 sha256=17509718dce2b9866d38a1ef388a8193f4f74caa74943f32a6a29b72a4f96b41\n","  Stored in directory: /root/.cache/pip/wheels/41/f3/10/985c450e02ed9288fbc5145e90e4726ae95399eaa612a55ee2\n","Successfully built keras-layer-normalization\n","Installing collected packages: keras-layer-normalization\n","Successfully installed keras-layer-normalization-0.16.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mModel: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," batch_normalization (BatchN  (None, 224, 224, 3)      12        \n"," ormalization)                                                   \n","                                                                 \n"," conv2d (Conv2D)             (None, 224, 224, 16)      448       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 112, 112, 16)     0         \n"," )                                                               \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 112, 112, 16)     64        \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 112, 112, 32)      4640      \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 56, 56, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 56, 56, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," dropout (Dropout)           (None, 56, 56, 32)        0         \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 56, 56, 64)        18496     \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 28, 28, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 28, 28, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_1 (Dropout)         (None, 28, 28, 64)        0         \n","                                                                 \n"," conv2d_3 (Conv2D)           (None, 28, 28, 128)       73856     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 14, 14, 128)      0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 14, 14, 128)      512       \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 14, 14, 256)       295168    \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 7, 7, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 7, 7, 256)        1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_3 (Dropout)         (None, 7, 7, 256)         0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 7, 7, 512)         1180160   \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 3, 3, 512)        0         \n"," 2D)                                                             \n","                                                                 \n"," batch_normalization_6 (Batc  (None, 3, 3, 512)        2048      \n"," hNormalization)                                                 \n","                                                                 \n"," dropout_4 (Dropout)         (None, 3, 3, 512)         0         \n","                                                                 \n"," global_average_pooling2d (G  (None, 512)              0         \n"," lobalAveragePooling2D)                                          \n","                                                                 \n"," dense (Dense)               (None, 1)                 513       \n","                                                                 \n","=================================================================\n","Total params: 1,577,325\n","Trainable params: 1,575,303\n","Non-trainable params: 2,022\n","_________________________________________________________________\n"]}],"source":["# !pip uninstall tensorflow\n","# !pip install tensorflow==1.4.0\n","!pip install tensorflow\n","\n","# !pip uninstall keras\n","# !pip install keras==2.0.8\n","from tensorflow import keras\n","from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n","from tensorflow.keras.layers import Dropout, Flatten, Dense\n","from tensorflow.keras import Sequential\n","from tensorflow import keras\n","# from tensorflow.keras import layers\n","# !pip install tensorflow.keras.layers.normalization\n","!pip install keras-layer-normalization\n","from tensorflow.keras.layers import BatchNormalization\n","from keras_layer_normalization import LayerNormalization\n","# from keras.layers.normalization import BatchNormalization\n","\n","# from LayerNormalization import BatchNormalization\n","\"\"\"\n","    Propsoed CNN architecture.\n","\n","\"\"\"\n","\n","model = Sequential()\n","\n","# Pamameters Initialization\n","input_shape = ( 224, 224, 3)\n","activation = 'relu'\n","padding = 'same'\n","droprate = 0.1\n","epsilon=0.001\n","\n","model = Sequential()\n","model.add(BatchNormalization(input_shape=input_shape))\n","model.add(Conv2D(filters=16, kernel_size=3, activation=activation, padding=padding))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(BatchNormalization(epsilon=epsilon))\n","\n","\n","model.add(Conv2D(filters=32, kernel_size=3, activation=activation, padding=padding))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(BatchNormalization(epsilon=epsilon))\n","model.add(Dropout(droprate))\n","\n","model.add(Conv2D(filters=64, kernel_size=3, activation=activation, padding=padding))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(BatchNormalization(epsilon=epsilon))\n","model.add(Dropout(droprate))\n","\n","model.add(Conv2D(filters=128, kernel_size=3, activation=activation, padding=padding))\n","model.add(MaxPooling2D(pool_size=2))\n","\n","model.add(BatchNormalization(epsilon=epsilon))\n","model.add(Dropout(droprate))\n","\n","model.add(Conv2D(filters=256, kernel_size=3, activation=activation, padding=padding))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(BatchNormalization(epsilon=epsilon))\n","model.add(Dropout(droprate))\n","\n","model.add(Conv2D(filters=512, kernel_size=3, activation=activation, padding=padding))\n","model.add(MaxPooling2D(pool_size=2))\n","model.add(BatchNormalization(epsilon=epsilon))\n","model.add(Dropout(droprate))\n","\n","model.add(GlobalAveragePooling2D())\n","#model.add(Flatten())\n","#model.add(Dense(256, kernel_initializer='glorot_normal', activation='relu'))\n","#model.add(Dropout(0.5))\n","\n","#model.add(Dense(128, kernel_initializer='glorot_normal', activation='relu'))\n","#model.add(Dropout(0.5))\n","#model.add(Dropout(droprate))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.summary() # Summary of the architecture"]},{"cell_type":"markdown","metadata":{"id":"Pv5PLYf-DvGu"},"source":["## Model Compile"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T20:36:06.367888Z","iopub.status.busy":"2023-05-01T20:36:06.367517Z","iopub.status.idle":"2023-05-01T20:36:06.383938Z","shell.execute_reply":"2023-05-01T20:36:06.382823Z","shell.execute_reply.started":"2023-05-01T20:36:06.367856Z"},"id":"OCzLuZ9xDvGu","trusted":true},"outputs":[],"source":["# Parameters Initialization\n","from tensorflow.keras.optimizers import RMSprop , Adam\n","# from tensorflow.python.keras.optimizers import rmsprop,SGD,Adam,Adadelta\n","\n","#opt = rmsprop(lr=0.0001, decay=1e-6)\n","\n","model.compile(loss='binary_crossentropy',optimizer=Adam(), metrics=['accuracy'])"]},{"cell_type":"markdown","metadata":{"id":"TocJbTX_DvGv"},"source":["## Load Data"]},{"cell_type":"markdown","metadata":{"id":"_cA7m6PCDvGw"},"source":["## Train Model"]},{"cell_type":"markdown","metadata":{"id":"7ZgwH0VkDvGx"},"source":["### Train Model on Original Data"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-05-01T20:36:11.262420Z","iopub.status.busy":"2023-05-01T20:36:11.261463Z","iopub.status.idle":"2023-05-01T20:43:24.368710Z","shell.execute_reply":"2023-05-01T20:43:24.367727Z","shell.execute_reply.started":"2023-05-01T20:36:11.262366Z"},"id":"WLnUSItJDvGy","outputId":"044a6640-4d8e-44e2-9cd4-392d3326339d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["2023-05-01 20:36:14.004067: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape insequential_1/dropout/dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"]},{"name":"stdout","output_type":"stream","text":["51/51 [==============================] - ETA: 0s - loss: 0.9483 - accuracy: 0.5232\n","Epoch 1: val_loss improved from inf to 0.74498, saving model to weights.custom.best.hdf5\n","51/51 [==============================] - 40s 587ms/step - loss: 0.9483 - accuracy: 0.5232 - val_loss: 0.7450 - val_accuracy: 0.5306\n","Epoch 2/20\n","51/51 [==============================] - ETA: 0s - loss: 0.6997 - accuracy: 0.5585\n","Epoch 2: val_loss improved from 0.74498 to 0.71929, saving model to weights.custom.best.hdf5\n","51/51 [==============================] - 23s 459ms/step - loss: 0.6997 - accuracy: 0.5585 - val_loss: 0.7193 - val_accuracy: 0.5306\n","Epoch 3/20\n","51/51 [==============================] - ETA: 0s - loss: 0.6741 - accuracy: 0.5963\n","Epoch 3: val_loss did not improve from 0.71929\n","51/51 [==============================] - 23s 448ms/step - loss: 0.6741 - accuracy: 0.5963 - val_loss: 0.7665 - val_accuracy: 0.5306\n","Epoch 4/20\n","51/51 [==============================] - ETA: 0s - loss: 0.6589 - accuracy: 0.6037\n","Epoch 4: val_loss did not improve from 0.71929\n","51/51 [==============================] - 23s 443ms/step - loss: 0.6589 - accuracy: 0.6037 - val_loss: 0.7515 - val_accuracy: 0.5306\n","Epoch 5/20\n","51/51 [==============================] - ETA: 0s - loss: 0.6390 - accuracy: 0.6334\n","Epoch 5: val_loss did not improve from 0.71929\n","51/51 [==============================] - 23s 444ms/step - loss: 0.6390 - accuracy: 0.6334 - val_loss: 0.9483 - val_accuracy: 0.5306\n","Epoch 6/20\n","51/51 [==============================] - ETA: 0s - loss: 0.6241 - accuracy: 0.6440\n","Epoch 6: val_loss did not improve from 0.71929\n","51/51 [==============================] - 23s 453ms/step - loss: 0.6241 - accuracy: 0.6440 - val_loss: 0.8705 - val_accuracy: 0.4743\n","Epoch 7/20\n","51/51 [==============================] - ETA: 0s - loss: 0.6134 - accuracy: 0.6582\n","Epoch 7: val_loss improved from 0.71929 to 0.71655, saving model to weights.custom.best.hdf5\n","51/51 [==============================] - 23s 458ms/step - loss: 0.6134 - accuracy: 0.6582 - val_loss: 0.7166 - val_accuracy: 0.5770\n","Epoch 8/20\n","51/51 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.7170\n","Epoch 8: val_loss improved from 0.71655 to 0.70117, saving model to weights.custom.best.hdf5\n","51/51 [==============================] - 23s 455ms/step - loss: 0.5691 - accuracy: 0.7170 - val_loss: 0.7012 - val_accuracy: 0.5721\n","Epoch 9/20\n","51/51 [==============================] - ETA: 0s - loss: 0.5380 - accuracy: 0.7337\n","Epoch 9: val_loss did not improve from 0.70117\n","51/51 [==============================] - 23s 459ms/step - loss: 0.5380 - accuracy: 0.7337 - val_loss: 0.8022 - val_accuracy: 0.5403\n","Epoch 10/20\n","51/51 [==============================] - ETA: 0s - loss: 0.5251 - accuracy: 0.7443\n","Epoch 10: val_loss did not improve from 0.70117\n","51/51 [==============================] - 24s 462ms/step - loss: 0.5251 - accuracy: 0.7443 - val_loss: 0.7143 - val_accuracy: 0.5917\n","Epoch 11/20\n","51/51 [==============================] - ETA: 0s - loss: 0.4689 - accuracy: 0.7833\n","Epoch 11: val_loss did not improve from 0.70117\n","51/51 [==============================] - 22s 439ms/step - loss: 0.4689 - accuracy: 0.7833 - val_loss: 0.7871 - val_accuracy: 0.5795\n","Epoch 12/20\n","51/51 [==============================] - ETA: 0s - loss: 0.4524 - accuracy: 0.7944\n","Epoch 12: val_loss did not improve from 0.70117\n","51/51 [==============================] - 23s 447ms/step - loss: 0.4524 - accuracy: 0.7944 - val_loss: 0.7698 - val_accuracy: 0.6137\n","Epoch 13/20\n","51/51 [==============================] - ETA: 0s - loss: 0.3980 - accuracy: 0.8310\n","Epoch 13: val_loss did not improve from 0.70117\n","51/51 [==============================] - 23s 445ms/step - loss: 0.3980 - accuracy: 0.8310 - val_loss: 0.8424 - val_accuracy: 0.6112\n","Epoch 14/20\n","51/51 [==============================] - ETA: 0s - loss: 0.3596 - accuracy: 0.8526\n","Epoch 14: val_loss did not improve from 0.70117\n","51/51 [==============================] - 23s 454ms/step - loss: 0.3596 - accuracy: 0.8526 - val_loss: 1.0342 - val_accuracy: 0.5575\n","Epoch 15/20\n","51/51 [==============================] - ETA: 0s - loss: 0.2782 - accuracy: 0.8941\n","Epoch 15: val_loss did not improve from 0.70117\n","51/51 [==============================] - 23s 450ms/step - loss: 0.2782 - accuracy: 0.8941 - val_loss: 0.8486 - val_accuracy: 0.5966\n","Epoch 16/20\n","51/51 [==============================] - ETA: 0s - loss: 0.2949 - accuracy: 0.8873\n","Epoch 16: val_loss did not improve from 0.70117\n","51/51 [==============================] - 23s 452ms/step - loss: 0.2949 - accuracy: 0.8873 - val_loss: 0.8367 - val_accuracy: 0.6064\n","Epoch 17/20\n","51/51 [==============================] - ETA: 0s - loss: 0.2120 - accuracy: 0.9257\n","Epoch 17: val_loss did not improve from 0.70117\n","51/51 [==============================] - 24s 464ms/step - loss: 0.2120 - accuracy: 0.9257 - val_loss: 0.9239 - val_accuracy: 0.6308\n","Epoch 18/20\n","51/51 [==============================] - ETA: 0s - loss: 0.1691 - accuracy: 0.9406\n","Epoch 18: val_loss did not improve from 0.70117\n","51/51 [==============================] - 23s 456ms/step - loss: 0.1691 - accuracy: 0.9406 - val_loss: 1.1128 - val_accuracy: 0.6112\n"]}],"source":["from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping \n","\n","batch_size = 10\n","epochs = 20\n","\n","checkpointer = ModelCheckpoint(filepath='weights.custom.best.hdf5',\n","                               monitor='val_loss',verbose=1, save_best_only=True)\n","\n","early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n","hist = model.fit(train_generator,\n","                #  initial_epoch=10,\n","                    epochs=epochs, verbose=1, callbacks=[checkpointer,early_stopping],\n","                    validation_data=val_generator,\n","                     )\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Ly8DeUix_nYf"},"source":[]},{"cell_type":"markdown","metadata":{"id":"VEUYAuYpDvGz"},"source":["### Evaluate Model on Orgiginal Data"]},{"cell_type":"markdown","metadata":{"id":"sfXb1fHDDvG1"},"source":["### Model Evaluation on Augumented Data"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T20:44:39.677543Z","iopub.status.busy":"2023-05-01T20:44:39.677123Z","iopub.status.idle":"2023-05-01T20:44:39.748271Z","shell.execute_reply":"2023-05-01T20:44:39.747241Z","shell.execute_reply.started":"2023-05-01T20:44:39.677506Z"},"trusted":true},"outputs":[],"source":["model.load_weights('/kaggle/working/weights.custom.best.hdf5')"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T20:44:43.594997Z","iopub.status.busy":"2023-05-01T20:44:43.594592Z","iopub.status.idle":"2023-05-01T20:44:43.704637Z","shell.execute_reply":"2023-05-01T20:44:43.703568Z","shell.execute_reply.started":"2023-05-01T20:44:43.594963Z"},"id":"qJLGKhMmJWU_","trusted":true},"outputs":[],"source":["# model = ...  # Get model (Sequential, Functional Model, or Model subclass)\n","model.save('/kaggle/working/savedmodel.hdf5')"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:15:02.574326Z","iopub.status.busy":"2023-05-01T21:15:02.573548Z","iopub.status.idle":"2023-05-01T21:15:02.580858Z","shell.execute_reply":"2023-05-01T21:15:02.579587Z","shell.execute_reply.started":"2023-05-01T21:15:02.574285Z"},"trusted":true},"outputs":[],"source":["from PIL import Image\n","from tensorflow.keras.preprocessing import image\n","\n","def pred(image_p):\n","    img_path = image_p #'/kaggle/input/gray-yellow-r-lsp/Gray VS Yellow/test/Gray/WIN_20230418_06_44_46_Pro.jpg'\n","    # loads RGB image as PIL.Image.Image type\n","    img = image.load_img(img_path, target_size=(224, 224, 3))\n","    # convert PIL.Image.Image type to 3D tensor with shape (96, 96, 3)\n","    x = image.img_to_array(img)\n","    # convert 3D tensor to 4D tensor with shape (1, 96, 96, 3) and return 4D tensor\n","    x = np.expand_dims(x, axis=0)\n","    x=x/255\n","    return model.predict(x)\n"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:14:09.271928Z","iopub.status.busy":"2023-05-01T21:14:09.270918Z","iopub.status.idle":"2023-05-01T21:14:09.349515Z","shell.execute_reply":"2023-05-01T21:14:09.348565Z","shell.execute_reply.started":"2023-05-01T21:14:09.271888Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 19ms/step\n"]},{"data":{"text/plain":["array([[0.6285674]], dtype=float32)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","from tensorflow.keras.applications.vgg16 import preprocess_input\n","import numpy as np\n","img = load_img('/kaggle/input/deep-fake-gp/test/training_real/real_00018.jpg', target_size=(224, 224))\n","img_array = img_to_array(img)\n","img_array = np.expand_dims(img_array, axis=0)\n","img_array=img_array/255\n","model.predict(img_array)"]},{"cell_type":"markdown","metadata":{},"source":[" print(pred('/kaggle/input/deep-fake-gp/test/training_fake/mid_113_1100.jpg'))\n"," "]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:57:23.116202Z","iopub.status.busy":"2023-05-01T21:57:23.115741Z","iopub.status.idle":"2023-05-01T21:57:24.562797Z","shell.execute_reply":"2023-05-01T21:57:24.561699Z","shell.execute_reply.started":"2023-05-01T21:57:23.116157Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["==============\n","1/1 [==============================] - 0s 49ms/step\n","mid_397_1111.jpg  [0.25946933]\n","1/1 [==============================] - 0s 38ms/step\n","hard_232_0001.jpg  [0.6408263]\n","1/1 [==============================] - 0s 33ms/step\n","mid_360_1100.jpg  [0.36346555]\n","1/1 [==============================] - 0s 28ms/step\n","mid_183_1111.jpg  [0.67851704]\n","1/1 [==============================] - 0s 27ms/step\n","mid_113_1100.jpg  [0.65781164]\n","1/1 [==============================] - 0s 28ms/step\n","mid_307_1101.jpg  [0.49597174]\n","1/1 [==============================] - 0s 27ms/step\n","mid_40_1111.jpg  [0.40570378]\n","1/1 [==============================] - 0s 28ms/step\n","mid_479_1111.jpg  [0.63596225]\n","==============\n","1/1 [==============================] - 0s 26ms/step\n","real_01060.jpg  [0.78628725]\n","1/1 [==============================] - 0s 27ms/step\n","real_00551.jpg  [0.6616218]\n","1/1 [==============================] - 0s 26ms/step\n","real_00903.jpg  [0.59186]\n","1/1 [==============================] - 0s 27ms/step\n","real_00675.jpg  [0.61157554]\n","1/1 [==============================] - 0s 26ms/step\n","real_00225.jpg  [0.5650997]\n","1/1 [==============================] - 0s 26ms/step\n","real_00722.jpg  [0.64117306]\n","1/1 [==============================] - 0s 25ms/step\n","real_00982.jpg  [0.6497783]\n","1/1 [==============================] - 0s 25ms/step\n","real_00018.jpg  [0.6285674]\n","1/1 [==============================] - 0s 26ms/step\n","real_00815.jpg  [0.54828054]\n"]}],"source":["import os \n","for i in os.listdir(\"/kaggle/input/deep-fake-gp/test/\"):\n","    print(\"==============\")\n","    for ii in os.listdir(f'/kaggle/input/deep-fake-gp/test/{i}'):\n","        print(f\"{ii} \",pred(f'/kaggle/input/deep-fake-gp/test/{i}/{ii}')[0])"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:08:00.626882Z","iopub.status.busy":"2023-05-01T21:08:00.626493Z","iopub.status.idle":"2023-05-01T21:08:00.707260Z","shell.execute_reply":"2023-05-01T21:08:00.706120Z","shell.execute_reply.started":"2023-05-01T21:08:00.626848Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 22ms/step\n","[[1.]]\n"]}],"source":[" print(pred('/kaggle/input/deep-fake-gp/test/training_fake/mid_113_1100.jpg'))"]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:07:54.285761Z","iopub.status.busy":"2023-05-01T21:07:54.284685Z","iopub.status.idle":"2023-05-01T21:07:54.718561Z","shell.execute_reply":"2023-05-01T21:07:54.717555Z","shell.execute_reply.started":"2023-05-01T21:07:54.285711Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 211ms/step\n"]},{"data":{"text/plain":["array([[0.78628725],\n","       [0.6115755 ],\n","       [0.25946945],\n","       [0.40570366],\n","       [0.63596225],\n","       [0.66162163],\n","       [0.49597162],\n","       [0.36346534],\n","       [0.5650996 ],\n","       [0.6785169 ],\n","       [0.6285673 ],\n","       [0.6578115 ],\n","       [0.6497782 ],\n","       [0.6408262 ],\n","       [0.6411729 ],\n","       [0.5482806 ],\n","       [0.59186   ]], dtype=float32)"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["model.predict(test_generator)"]},{"cell_type":"code","execution_count":21,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:02:46.673957Z","iopub.status.busy":"2023-05-01T21:02:46.673189Z","iopub.status.idle":"2023-05-01T21:02:46.783425Z","shell.execute_reply":"2023-05-01T21:02:46.782454Z","shell.execute_reply.started":"2023-05-01T21:02:46.673916Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 17 images belonging to 2 classes.\n"]}],"source":["test_gen = ImageDataGenerator(rescale=1./255)\n","test_generator = test_gen.flow_from_directory(\n","        # This is the target directory\n","        \"/kaggle/input/deep-fake-gp/test\",\n","        # All images will be resized to 150x150\n","        target_size=(224, 224),\n","        batch_size=32,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary',\n","        #color_mode=\"grayscale\"\n","        )"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:37:15.718845Z","iopub.status.busy":"2023-05-01T21:37:15.717582Z","iopub.status.idle":"2023-05-01T21:37:15.773682Z","shell.execute_reply":"2023-05-01T21:37:15.772606Z","shell.execute_reply.started":"2023-05-01T21:37:15.718788Z"},"trusted":true},"outputs":[],"source":["import os \n","import shutil\n","os.mkdir(\"testt\")\n","for i in os.listdir(\"/kaggle/input/deep-fake-gp/test/\"):\n","    try:\n","        \n","        os.mkdir(f\"/kaggle/working/testt/{i}\")\n","    except:\n","        print(\"file exists\")\n","    for ii in os.listdir(f'/kaggle/input/deep-fake-gp/test/{i}'):\n","        shutil.copy(f'/kaggle/input/deep-fake-gp/test/{i}/{ii}',f'testt/{i}/{ii}')\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-05-01T21:38:06.054388Z","iopub.status.busy":"2023-05-01T21:38:06.053617Z","iopub.status.idle":"2023-05-01T21:38:07.111281Z","shell.execute_reply":"2023-05-01T21:38:07.110053Z","shell.execute_reply.started":"2023-05-01T21:38:06.054348Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  adding: kaggle/working/testt/ (stored 0%)\n","  adding: kaggle/working/testt/training_real/ (stored 0%)\n","  adding: kaggle/working/testt/training_real/real_01060.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00018.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00982.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00903.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00225.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00551.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00722.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00675.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_real/real_00815.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/ (stored 0%)\n","  adding: kaggle/working/testt/training_fake/mid_183_1111.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/mid_360_1100.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/mid_479_1111.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/mid_307_1101.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/mid_397_1111.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/mid_113_1100.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/mid_40_1111.jpg (deflated 0%)\n","  adding: kaggle/working/testt/training_fake/hard_232_0001.jpg (deflated 0%)\n"]}],"source":["!zip -r /kaggle/working/testt.zip /kaggle/working/testt/"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
